services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    environment:
      # Preconfigure the built-in OpenAI provider to use the sidecar.
      # Open WebUI stores these in persistent config on first boot.
      - OPENAI_API_BASE_URLS=http://open-persona-sidecar:8000/v1
      - OPENAI_API_KEYS=sk-open-persona
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data

  opencode:
    image: ghcr.io/sst/opencode:latest
    restart: unless-stopped
    # Image entrypoint is `opencode`, so command is subcommand args.
    command: ["serve", "--hostname", "0.0.0.0", "--port", "4096"]
    environment:
      # Configure providers by setting env vars here (PoC tradeoff A).
      # Example:
      # - ANTHROPIC_API_KEY=...
      # - OPENAI_API_KEY=...
      - XDG_CONFIG_HOME=/data/config
      - XDG_STATE_HOME=/data/state
    volumes:
      - opencode-data:/data
      - open-persona-workspaces:/workspace/open-persona
    working_dir: /workspace/open-persona

  open-persona-sidecar:
    build:
      context: ./services/open-persona-sidecar
    restart: unless-stopped
    environment:
      - PORT=8000
      - OPENCODE_BASE_URL=http://opencode:4096
      - WORKSPACE_ROOT=/workspace/open-persona
      # Runner mode:
      # - shared: use the single `opencode` service
      # - container: spawn `opencode serve` runner containers (requires docker socket)
      - RUNNER_MODE=container
      - RUNNER_NETWORK=open-persona_default
      - RUNNER_IMAGE=ghcr.io/sst/opencode:latest
      - WORKSPACE_VOLUME=open-persona_open-persona-workspaces
      - OPENCODE_DATA_VOLUME=open-persona_opencode-data
    volumes:
      - open-persona-workspaces:/workspace/open-persona
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8000:8000"
    depends_on:
      - opencode

volumes:
  open-webui:
  opencode-data:
  open-persona-workspaces:
