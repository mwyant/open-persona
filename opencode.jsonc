{
  "$schema": "https://opencode.ai/config.json",
  // Per-project provider pointing at your LM Studio server.
  // Model IDs must match what LM Studio returns from GET /v1/models.
  "provider": {
    "lmstudio-local": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (Mike LAN)",
      "options": {
        "baseURL": "http://192.168.1.196:1234/v1",
        // LM Studio usually doesn't require a key, but OpenAI‑compatible clients
        // often expect one to be present.
        "apiKey": "lm-studio"
      },
      "models": {
        "openai/gpt-oss-20b": {
          "name": "gpt-oss-20b (local)",
          // Updated context limit for gpt‑oss
          "limit": { "context": 131072, "output": 8192 }
        },
        "nvidia/nemotron-3-nano": {
          "name": "NVIDIA Nemotron-3-nano (local)",
          "limit": { "context": 213592, "output": 16384 }
        },

        "glm-4.6v-flash": {
          "name": "glm-4.6v-flash (local)",
          "limit": { "context": 32768, "output": 8192 }
        },
        "qwen/qwen3-vl-30b": {
          "name": "Qwen 3 V1 30B (local)",
          "limit": { "context": 132719, "output": 8192 }
        },
        "text-embedding-nomic-embed-text-v1.5": {
          "name": "Nomic Embed Text v1.5 (local embedding)"
        }
      }
    }
  },
  // Default models for this project.
  // Build/Plan will use the gpt‑oss model by default.
  // All subagent tasks will use glm‑4.6v‑flash as the small model.
  "model": "qwen/qwen3-vl-30b",
  "small_model": "lmstudio-local/glm-4.6v-flash"
}
